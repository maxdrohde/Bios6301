---
title: 'Bios 6301: Assignment 3'
author: "Max Rohde"
output:
  html_document: default
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
```

### Question 1 ###

**15 points**

Write a simulation to calculate the power for the following study
design.  The study has two variables, treatment group and outcome.
There are two treatment groups (0, 1) and they should be assigned
randomly with equal probability.  The outcome should be a random normal
variable with a mean of 60 and standard deviation of 20.  If a patient
is in the treatment group, add 5 to the outcome.  5 is the true
treatment effect.  Create a linear model for the outcome by the
treatment group, and extract the p-value (hint: see assigment1).
Test if the p-value is less than or equal to the alpha level, which
should be set to 0.05.

Repeat this procedure 1000 times. The power is calculated by finding
the percentage of times the p-value is less than or equal to the alpha
level.  Use the `set.seed` command so that the professor can reproduce
your results.

1. Find the power when the sample size is 100 patients. (10 points)

1. Find the power when the sample size is 1000 patients. (5 points)

```{r}
set.seed(27182)

simulate <- function(n){
    # Create the group assignment by using a random permutation
    groups <- sample(rep(c("Treatment", "Control"), n/2))
    
    # Create a data frame to organize our results
    df <- tibble(subject_id =1:n,
                 group = groups,
                 outcome = rnorm(n, mean=60, sd=20))
    
    # Add 5 to the outcome for all subjects in the treatment group
    df <- mutate(df,
                 outcome = ifelse(group=="Treatment", outcome+5, outcome))
    
    # Create the linear model
    model <- lm(outcome ~ group, data = df) 
    
    # Get the p-value from the model
    p <- summary(model)$coefficients[[2,4]]
    
    # return TRUE if statistically significant, FALSE otherwise
    return(p<=0.05)
}

# Simulate 1000 times for studies of 100 subjects
power_n100 <- map_lgl(1:1000, ~simulate(n=100)) %>% mean()

# Simulate 1000 times for studies of 1000 subjects
power_n1000 <- map_lgl(1:1000, ~simulate(n=1000)) %>% mean()
```

```
The estimated power with 100 subjects is 0.238
The estimated power with 1000 subjects is 0.974
```

### Question 2 ###

**14 points**

Obtain a copy of the [football-values lecture](https://github.com/couthcommander/football-values).
Save the `2020/proj_wr20.csv` file in your working directory.  Read
in the data set and remove the first two columns.

1. Show the correlation matrix of this data set. (4 points)

```{r,  message=FALSE}
# Read in the data
df <- read_csv('https://raw.githubusercontent.com/couthcommander/football-values/master/2020/proj_wr20.csv')

# Remove first two columna
select(df, -(1:2)) -> df

# Show correlation matrix
cor(df)
```

1. Generate a data set with 30 rows that has a similar correlation
structure.  Repeat the procedure 1,000 times and return the mean
correlation matrix. (10 points)

```{r}
# Store the column means and covariance matrix
means <- colMeans(df)
covariance <- var(df)

# This generates one data set, with 30 rows, that has similar correlation structure
# to the original dataset. We do this simulating a multivariate normal random variable
# with the same mean vector and covariance matrix
MASS::mvrnorm(30, mu=means, Sigma=covariance)
```


```{r}
# Repeat the above simulation 1000 times, and store each matrix in a list
simulated_datasets <- map(1:1000, ~MASS::mvrnorm(30, mu=means, Sigma=covariance))

# Take the correlation of all the simulated matrices
simulated_correlation_matrices <- map(simulated_datasets, ~cor(.x))

# Take the average of all the simulated correlation matrices by summing them up
# and dividing by the total number of them
mean_correlation_matrix <- reduce(simulated_correlation_matrices, `+`) / length(simulated_correlation_matrices)

# View the mean correlation matrix from the 1000 simulations
mean_correlation_matrix
```

### Question 3 ###

**21 points**

Here's some code:

```{r}
nDist <- function(n = 100) {
    df <- 10
    prob <- 1/3
    shape <- 1
    size <- 16
    list(
        beta = rbeta(n, shape1 = 5, shape2 = 45),
        binomial = rbinom(n, size, prob),
        chisquared = rchisq(n, df),
        exponential = rexp(n),
        f = rf(n, df1 = 11, df2 = 17),
        gamma = rgamma(n, shape),
        geometric = rgeom(n, prob),
        hypergeometric = rhyper(n, m = 50, n = 100, k = 8),
        lognormal = rlnorm(n),
        negbinomial = rnbinom(n, size, prob),
        normal = rnorm(n),
        poisson = rpois(n, lambda = 25),
        t = rt(n, df),
        uniform = runif(n),
        weibull = rweibull(n, shape)
    )
}
```

1. What does this do? (3 points)

    ```{r}
    round(sapply(nDist(500), mean), 2)
    ```
    
```
The above code takes samples of size 500 from the distribtions defined in nDist() and computes the mean of each sample, then rounds the answer to two decimal places.
```

1. What about this? (3 points)

```{r}
    sort(apply(replicate(20, round(sapply(nDist(10000), mean), 2)), 1, sd))
```

```
The above code takes samples of size 10000 from the distribtions defined in nDist() and computes the mean of each sample, then rounds the answer to two decimal places, and repeats this 20 times so that in the end, we have 20 means per distribion. Then the standard devitation of each of these 20 means is taken and then sorted from smallest to largest.
```

```
In the output above, a small value would indicate that `N=10,000` would provide a sufficent sample size as to estimate the mean of the distribution. Let's say that a value *less than 0.02* is "close enough".
```

The below code estimates the sufficient sample size to estimate the mean for each distribution.

For each distribution, the procedure described above is computed starting from a sample size of 2, and the standard deviation of the 20 means is then stored. Until the standard deviation of the 20 means is below 0.02, the while loop continues, increasing the sample size each time. The first sample size to produce a standard deviation less than 0.02 is a rough estimate of the sufficient sample size to estimate the mean of the distribution.

```{r}
df <- 10
prob <- 1/3
shape <- 1
size <- 16

l <- list(
        beta = function(n) rbeta(n, shape1 = 5, shape2 = 45),
        binomial = function(n) rbinom(n, size, prob),
        chisquared = function(n) rchisq(n, df),
        exponential = function(n) rexp(n),
        f = function(n) rf(n, df1 = 11, df2 = 17),
        gamma = function(n) rgamma(n, shape),
        geometric = function(n) rgeom(n, prob),
        hypergeometric = function(n) rhyper(n, m = 50, n = 100, k = 8),
        lognormal = function(n) rlnorm(n),
        negbinomial = function(n) rnbinom(n, size, prob),
        normal = function(n) rnorm(n),
        poisson = function(n) rpois(n, lambda = 25),
        t = function(n) rt(n, df),
        uniform = function(n) runif(n),
        weibull = function(n) rweibull(n, shape)
    )
```

```{r}
######################################################
# Simulation to estimate the sufficient sample size
# to estimate the mean of each distribution
#
# This code may take a few minutes to run!
######################################################
set.seed(31415)

for (func in l){ # loop over all the distributions
    n <- 2
    stdev <- 1
    while (stdev > 0.02) # Loop until desired precision
    {
        # Simulate the data and take the standard deviation
        stdev <- map_dbl(1:20, ~func(n) %>% mean()) %>% sd()
        
        # Adaptively adjust the step size
        if(n <500){n <- n+1}
        else if(n < 1000){n <- n+10}
        else if(n < 10000){n <- n+50}
        else{n <- n+100}
    }
    print(func)
    print(n)
    print(stdev)
}
```

1. For each distribution, estimate the sample size required to simulate the distribution's mean. (15 points)

Don't worry about being exact. It should already be clear that N < 10,000 for many of the distributions. You don't have to show your work. Put your answer to the right of the vertical bars (`|`) below.

distribution|N
---|---
beta| 5
binomial| 5,000
chisquared| 20,000
exponential| 2,000
f| 500
gamma| 2,000
geometric| 8,000
hypergeometric| 2,000
lognormal| 10,000
negbinomial| 80,000
normal| 2,000
poisson| 35,000
t| 2,000
uniform| 100
weibull| 1,500
